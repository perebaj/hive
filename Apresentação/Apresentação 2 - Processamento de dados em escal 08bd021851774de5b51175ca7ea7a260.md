# Apresenta√ß√£o 2 - Processamento de dados em escala

<aside>
üí° O objetivo da apresenta√ß√£o √© mostrar na pr√°tica as funcionalidades do Hive

</aside>

<aside>
üí° Lembrando um pouco sobre o que foi mostrado na primeira apresenta√ß√£o

</aside>

- Data Warehouse

# Hive Introduction

- Artigo (**Hive - a petabyte scale data warehouse using Hadoop - 2010**)

Alguns pontos chave:

- O tamanho dos conjuntos de dados coletados e analisados no setor de intelig√™ncia de neg√≥cios est√° crescendo rapidamente, tornando as solu√ß√µes de armazenamento tradicionais proibitivamente caras.
- A an√°lise escal√°vel em grandes conjuntos de dados tem sido fundamental para as fun√ß√µes de v√°rias equipes do Facebook - tanto de engenharia quanto de n√£o engenharia.
- Toda a infraestrutura de processamento de dados no Facebook antes de 2008 foi usando um RDBMS comercial.

<aside>
üí°  Os dados que o Facebook estava gerando, estava crescendo muito r√°pido - como exemplo, crescemos de um conjunto de dados de 15 TB em 2007 para um conjunto de dados de 700 TB hoje(2010).

</aside>

- Hadoop √© uma implementa√ß√£o popular de MapReduce, que possui c√≥digo aberto e que estava sendo amplamente utilizado em empresas como Yahoo, Facebook.
- No entanto, o modelo de programa√ß√£o MapReduce √© de n√≠vel muito baixo e exige que os desenvolvedores escrevam programas personalizados que s√£o dif√≠ceis de manter e reutilizar.
- **Hive, √© uma solu√ß√£o de armazenamento de dados de c√≥digo aberto constru√≠da sobre o Hadoop.**
- O Hive suporta consultas expressas em uma linguagem declarativa como SQL - HiveQL
    - O HiveQL √© compilado, por debaixo dos panos, com sintaxes de MapReduce que s√£o executadas usando o Hadoop.
    - O HiveQL permite que os usu√°rios conectem scripts personalizados de MapReduce em consultas SQL
    - A linguagem inclui um **sistema de tipos** com suporte para **tabelas** contendo tipos **primitivos**, cole√ß√µes como **arrays** e **mapas** e **composi√ß√µes aninhadas** dos mesmos.
    - Hive tamb√©m inclui um **cat√°logo do sistema** - **Metastore** - que cont√©m esquemas e estat√≠sticas, √∫teis na explora√ß√£o de dados, otimiza√ß√£o de consultas e compila√ß√£o de consultas.

## Motiva√ß√£o:

O fato de o Hadoop j√° ser um projeto de c√≥digo aberto que estava sendo usado em escala de petabytes e fornecer escalabilidade usando hardware comum, foi uma proposta muito atraente para o Facebook desenvolver o Hive.

## Se√ß√µes que vamos abordar

- A se√ß√£o II descreve o modelo de dados, os sistemas de tipos e o HiveQL. ‚úÖ
- A Se√ß√£o III detalha como os dados nas tabelas do Hive s√£o armazenados no sistema de arquivos distribu√≠do subjacente ‚Äì HDFS (sistema de arquivos Hadoop). ‚úÖ
- A Se√ß√£o IV descreve a arquitetura do sistema e v√°rios componentes do Hive. ‚úÖ

# Sess√£o 2

<aside>
üí° Palavras reservadas do HiveQL: [https://cwiki.apache.org/confluence/display/hive/languagemanual+ddl](https://cwiki.apache.org/confluence/display/hive/languagemanual+ddl)

</aside>

## üçú Data Model and Type System

Semelhante aos bancos de dados tradicionais, o Hive armazena dados em tabelas, onde cada tabela consiste em um n√∫mero de linhas e cada linha consiste em um n√∫mero especificado de colunas. Cada coluna tem um tipo associado. O tipo √© um **tipo primitivo** ou um **tipo complexo**. Atualmente, os seguintes tipos primitivos s√£o suportados:

- Inteiros ‚Äì bigint(8 bytes), int(4 bytes), smallint(2 bytes), tinyint(l byte). Todos os tipos inteiros s√£o assinados.
- N√∫meros de ponto flutuante ‚Äì float (precis√£o simples), double (precis√£o dupla)
- Corda

O Hive tamb√©m oferece suporte nativo aos seguintes tipos complexos:

- Arrays associativos ‚Äì map<key-type, value-type>
- Listas ‚Äì lista<tipo de elemento>
- Structs ‚Äì struct<file-name: field-type, ‚Ä¶ >

### üôÑ Serializa√ß√£o e Desserializa√ß√£o

Em certas situa√ß√µes, as informa√ß√µes de tipo tamb√©m podem ser fornecidas por um arquivo jar, fornecendo uma implementa√ß√£o correspondente da interface java ObjectInspector e expondo essa implementa√ß√£o atrav√©s do m√©todo getObjectInspector presente na interface SerDe.

Assim √© poss√≠vel importar pacotes de Serializa√ß√£o e Desserializa√ß√£o de dados para cria√ß√£o de tabelas personalizadas.

![Untitled](Apresentac%CC%A7a%CC%83o%202%20-%20Processamento%20de%20dados%20em%20escal%2008bd021851774de5b51175ca7ea7a260/Untitled.png)

## Query Language

Recursos SQL tradicionais como subconsultas de cl√°usulas, v√°rios tipos de jun√ß√µes - jun√ß√µes internas, externas esquerdas, jun√ß√µes externas e externas direitas, produtos cartesianos, agrupar bys e agrega√ß√µes, uni√£o de todos, criar tabela como sele√ß√£o e muitas fun√ß√µes √∫teis em tipos primitivos e complexos tornar a linguagem muito parecida com SQL.

### Limita√ß√µes

Outra limita√ß√£o est√° em como as inser√ß√µes s√£o feitas. Atualmente, o Hive n√£o oferece suporte √† inser√ß√£o em uma tabela ou parti√ß√£o de dados existente e todas as inser√ß√µes substituem os dados existentes. Assim, tornamos isso expl√≠cito em nossa sintaxe da seguinte forma:

Na realidade, essas restri√ß√µes n√£o s√£o um problema. Raramente vimos um caso em que a consulta n√£o pode ser expressa como um equi-join e, como a maioria dos dados √© carregada em nosso warehouse diariamente ou a cada hora, simplesmente carregamos os dados em uma nova parti√ß√£o da tabela para esse dia ou hora.

## MapReduce no SQL ü§Æ

Al√©m dessas restri√ß√µes, o HiveQL possui extens√µes para suportar an√°lises expressas como programas de MapReduce pelos usu√°rios e na linguagem de programa√ß√£o de sua escolha. Isso permite que usu√°rios avan√ßados expressem l√≥gica complexa em programas de redu√ß√£o de MapReduce as consultas HiveQL perfeitamente. √Äs vezes, essa pode ser a √∫nica abordagem razo√°vel, por exemplo. No caso de existirem bibliotecas em python, PHP ou qualquer outra linguagem que o usu√°rio queira utilizar para transforma√ß√£o de dados. O exemplo de contagem de palavras can√¥nicas em uma tabela de documentos pode, por exemplo, ser expresso usando map-reduce da seguinte maneira:

![Untitled](Apresentac%CC%A7a%CC%83o%202%20-%20Processamento%20de%20dados%20em%20escal%2008bd021851774de5b51175ca7ea7a260/Untitled%201.png)

Como mostrado neste exemplo, a cl√°usula MAP indica como as colunas de entrada (doctext neste caso) podem ser transformadas usando um programa de usu√°rio (neste caso ‚Äòpython we_mapper.py‚Äô) em colunas de sa√≠da (word e cnt). A cl√°usula CLUSTER BY na subconsulta especifica as colunas de sa√≠da, criptografadas para distribuir os dados para os redutores e, finalmente, a cl√°usula REDUCE especifica o programa do usu√°rio a ser invocado (python wc_reduce.py neste caso) nas colunas de sa√≠da do subconsulta.

# Se√ß√£o 3 - **Data¬†Storage, Serde¬†And¬†File¬†Formats**

## Data Storage

Enquanto as tabelas s√£o unidades de dados l√≥gicos no Hive, os metadados da tabela associam os dados em uma tabela aos diret√≥rios HDFS. As unidades de dados prim√°rias e seus mapeamentos no espa√ßo de nomes HDFS s√£o os seguintes:

- Tabelas _ Uma tabela √© armazenada em um diret√≥rio em HDFS.
- Parti√ß√µes _ Uma parti√ß√£o da tabela √© armazenada em um subdiret√≥rio dentro do diret√≥rio de uma tabela.
- Buckets _ Um bucket √© armazenado em um arquivo dentro do diret√≥rio da parti√ß√£o ou da tabela, dependendo se a tabela √© particionada ou n√£o.

Uma tabela pode ser particionada ou n√£o particionada. Uma tabela particionada pode ser criada especificando a cl√°usula PARTITIONED BY na instru√ß√£o CREATE TABLE conforme mostrado abaixo.

![Untitled](Apresentac%CC%A7a%CC%83o%202%20-%20Processamento%20de%20dados%20em%20escal%2008bd021851774de5b51175ca7ea7a260/Untitled%202.png)

O conceito de unidade de armazenamento final que o Hive usa √© o conceito de Baldes. Um bucket √© um arquivo dentro do diret√≥rio de n√≠vel folha de uma tabela ou parti√ß√£o. No momento em que a tabela √© criada, o usu√°rio pode especificar o n√∫mero de buckets necess√°rios e a coluna na qual armazenar os dados. Na implementa√ß√£o atual, essa informa√ß√£o √© usada para podar os dados caso o usu√°rio execute a consulta em uma amostra de dados, por exemplo. Uma tabela agrupada em 32 buckets pode gerar rapidamente uma amostra de 1/32, escolhendo examinar o primeiro bucket de dados.

![Untitled](Apresentac%CC%A7a%CC%83o%202%20-%20Processamento%20de%20dados%20em%20escal%2008bd021851774de5b51175ca7ea7a260/Untitled%203.png)

### External Table

Uma tabela externa difere de uma tabela normal apenas porque um comando drop table em uma tabela externa apenas descarta os metadados da tabela e n√£o exclui nenhum dado. Uma queda em uma tabela normal, por outro lado, tamb√©m descarta os dados associados √† tabela.

![Untitled](Apresentac%CC%A7a%CC%83o%202%20-%20Processamento%20de%20dados%20em%20escal%2008bd021851774de5b51175ca7ea7a260/Untitled%204.png)

## Serialization/Deserialization (SerDe)

Como mencionado anteriormente, o Hive pode pegar uma implementa√ß√£o da interface java SerDe fornecida pelo usu√°rio e associ√°-la a uma tabela ou parti√ß√£o. Como resultado, os formatos de dados personalizados podem ser facilmente interpretados e consultados.

# Se√ß√£o 4 - **System¬†Architecture¬†And¬†Components**

![Untitled](Apresentac%CC%A7a%CC%83o%202%20-%20Processamento%20de%20dados%20em%20escal%2008bd021851774de5b51175ca7ea7a260/Untitled%205.png)

## Componentes arquiteturais do Hive

Os seguintes componentes s√£o os principais blocos de constru√ß√£o no Hive:

- **Metastore** ‚Äì O componente que armazena o cat√°logo do sistema e metadados sobre tabelas, colunas, parti√ß√µes, etc.
- **Driver** ‚Äì O componente que gerencia o ciclo de vida de uma instru√ß√£o HiveQL conforme ela se move pelo Hive. O driver tamb√©m mant√©m um identificador de sess√£o e todas as estat√≠sticas de sess√£o.
- **Query Compiler** ‚Äì O componente que compila o HiveQL em um gr√°fico ac√≠clico direcionado de tarefas de mapeamento/redu√ß√£o.
- **Execution Engine** ‚Äì O componente que executa as tarefas produzidas pelo compilador na ordem de depend√™ncia adequada. O mecanismo de execu√ß√£o interage com a inst√¢ncia subjacente do Hadoop.
- **HiveServer** ‚Äì O componente que fornece uma interface thrift e um servidor JDBC/ODBC e fornece uma maneira de integrar o Hive com outros aplicativos.
- Componentes de clientes como a **interface de linha de comando** (CLI), a interface do usu√°rio da web e o driver JDBC/ODBC.